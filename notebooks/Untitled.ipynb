{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b85b5b-e7a5-4814-a559-63a0dbef74cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data DATA [--target TARGET]\n",
      "                             [--sampleID SAMPLEID] [--output OUTPUT]\n",
      "                             [--seed SEED] [--test_size TEST_SIZE]\n",
      "                             [--return_split]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vpp1/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to load data and preprocess it\n",
    "def load_and_preprocess_data(file_path, target_column='label', id_column='SampleID'):\n",
    "    df = pd.read_csv(file_path)  # Load the dataset\n",
    "    print(f\"Data shape: {df.shape}\")  # Print the shape of the dataset\n",
    "    print(f\"Original target column values:\\n{df[target_column].head()}\")\n",
    "\n",
    "    # Filter out the id column\n",
    "    if id_column in df.columns:\n",
    "        df = df.drop(columns=[id_column])\n",
    "        print(f\"Data shape after dropping id column: {df.shape}\")\n",
    "\n",
    "    # Ensure all columns except target are numeric\n",
    "    for col in df.columns:\n",
    "        if col != target_column:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Fill missing values with the mean of each column\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    \n",
    "    # Convert target column to numeric if it's categorical\n",
    "    if df[target_column].dtype == 'object' or df[target_column].dtype.name == 'category':\n",
    "        df[target_column], unique_vals = pd.factorize(df[target_column])\n",
    "        print(f\"Factorized target column values:\\n{df[target_column].head()}\")\n",
    "        print(f\"Mapping: {dict(enumerate(unique_vals))}\")\n",
    "    \n",
    "    # Print the unique values to verify factorization\n",
    "    print(f\"Unique values in target column after factorization: {df[target_column].unique()}\")\n",
    "    print(f\"Processed data:\\n{df.head()}\")  # Print the first few rows to ensure proper processing\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_prep_for_ae(file_path, id_column='SampleID'):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the dataset for autoencoder usage, where there is no target column, \n",
    "    and only feature columns are present.\n",
    "\n",
    "    :param file_path: Path to the CSV file containing the dataset.\n",
    "    :param id_column: Column name that represents the sample IDs, which will be dropped if present.\n",
    "    :return: Preprocessed DataFrame with only feature columns.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Data shape: {df.shape}\")  # Print the shape of the dataset\n",
    "\n",
    "    # Drop the ID column if it exists\n",
    "    if id_column in df.columns:\n",
    "        df = df.drop(columns=[id_column])\n",
    "        print(f\"Data shape after dropping ID column: {df.shape}\")\n",
    "\n",
    "    # Ensure all columns are numeric\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Fill missing values with the mean of each column\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    \n",
    "    print(f\"Processed data (first few rows):\\n{df.head()}\")  # Print the first few rows to ensure proper processing\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to split the data into training and testing sets, separately for each class\n",
    "def split_data(df, target_column, test_size=0.2, random_state=None):\n",
    "    # Check if the target column has been correctly processed\n",
    "    print(f\"Unique values in target column: {df[target_column].unique()}\")\n",
    "\n",
    "    # Separate the data by class\n",
    "    df_class_0 = df[df[target_column] == 0]\n",
    "    df_class_1 = df[df[target_column] == 1]\n",
    "\n",
    "    print(f\"Class 0 samples: {len(df_class_0)}, Class 1 samples: {len(df_class_1)}\")\n",
    "\n",
    "    # Ensure there are enough samples to split\n",
    "    if len(df_class_0) == 0 or len(df_class_1) == 0:\n",
    "        raise ValueError(\"One of the classes has no samples, cannot perform train/test split.\")\n",
    "\n",
    "    # Split each class individually, retaining indices\n",
    "    X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(\n",
    "        df_class_0.drop(columns=[target_column]), \n",
    "        df_class_0[target_column], \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "        df_class_1.drop(columns=[target_column]), \n",
    "        df_class_1[target_column], \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Combine the splits back together\n",
    "    X_train = pd.concat([X_train_0, X_train_1])\n",
    "    y_train = pd.concat([y_train_0, y_train_1])\n",
    "    X_test = pd.concat([X_test_0, X_test_1])\n",
    "    y_test = pd.concat([y_test_0, y_test_1])\n",
    "\n",
    "    print(f\"Train set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    print(X_train)\n",
    "    \n",
    "    return {'train': {'X': X_train, 'y': y_train},\n",
    "            'test': {'X': X_test, 'y': y_test}}\n",
    "\n",
    "\n",
    "def split_data_for_ae(df, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits the data into training and test sets for autoencoder usage.\n",
    "\n",
    "    :param df: DataFrame, where the columns are the feature names and the index represents sample IDs.\n",
    "    :param test_size: Proportion of data to include in the test set.\n",
    "    :param random_state: Seed for reproducibility of the split.\n",
    "    :return: Dictionary with 'train' and 'test' keys, each containing a DataFrame with the features.\n",
    "    \"\"\"\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Print the sizes of the training and test sets\n",
    "    print(f\"Train set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # Return a dictionary with the split data\n",
    "    return {'train': {'X': X_train},\n",
    "            'test': {'X': X_test}}\n",
    "\n",
    "\n",
    "# Function to save the training and testing data into a specified output directory\n",
    "def save_split_data(split_data_dict, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    preprocessed_file_train_X = os.path.join(output_dir, 'train_X.csv')\n",
    "    preprocessed_file_train_y = os.path.join(output_dir, 'train_y.csv')\n",
    "    preprocessed_file_test_X = os.path.join(output_dir, 'test_X.csv')\n",
    "    preprocessed_file_test_y = os.path.join(output_dir, 'test_y.csv')\n",
    "    \n",
    "    # Save the data with indices (sample names)\n",
    "    split_data_dict['train']['X'].to_csv(preprocessed_file_train_X)\n",
    "    split_data_dict['train']['y'].to_csv(preprocessed_file_train_y)\n",
    "    split_data_dict['test']['X'].to_csv(preprocessed_file_test_X)\n",
    "    split_data_dict['test']['y'].to_csv(preprocessed_file_test_y)\n",
    "\n",
    "    print(f\"Training and testing data saved to {output_dir}\")\n",
    "\n",
    "# Command-line argument parser\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocess the dataset and optionally save the split data.\")\n",
    "    parser.add_argument('--data', type=str, required=True, help='Path to the input CSV file.')\n",
    "    parser.add_argument('--target', type=str, default='label', help='Target column name in the dataset.')\n",
    "    parser.add_argument('--sampleID', type=str, default='sampleID', help='SampleID column name in the dataset.')\n",
    "    parser.add_argument('--output', type=str, help='Directory to save the preprocessed data (optional).')\n",
    "    parser.add_argument('--seed', type=int, default=None, help='Seed for random state of split.')\n",
    "    parser.add_argument('--test_size', type=float, default=0.2, help='Proportion of the dataset to include in the test split.')\n",
    "    parser.add_argument('--return_split', action='store_true', help='If set, the function will return the split data instead of saving it.')\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_arguments()\n",
    "\n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(args.data, args.target,args.sampleID)\n",
    "\n",
    "    # Split the data with the provided test size and seed\n",
    "    split_data_dict = split_data(df, args.target, test_size=args.test_size, random_state=args.seed)\n",
    "\n",
    "    # Save the split data if an output directory is specified\n",
    "    if args.output:\n",
    "        save_split_data(split_data_dict, args.output)\n",
    "    \n",
    "    # If return_split is set, return the data (useful for real-time splits during model training)\n",
    "    if args.return_split:\n",
    "        train_X, train_y = split_data_dict['train']['X'], split_data_dict['train']['y']\n",
    "        test_X, test_y = split_data_dict['test']['X'], split_data_dict['test']['y']\n",
    "        print(\"Returning split data for further processing.\")\n",
    "        # Returning as dataframes with indices\n",
    "        # In an actual implementation, you would typically handle this differently, depending on how you want to use the split data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a9a10-d42d-4122-879a-082b43412a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
